{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7aae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import gensim.downloader as api\n",
    "\n",
    "from processing import normalize_text\n",
    "\n",
    "print('Loading word2vec...')\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "class DomainWordScore:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, word_distribution, trim_percentile=0.9*100):\n",
    "        self.word_distribution = word_distribution\n",
    "        self.trim_percentile = trim_percentile\n",
    "\n",
    "    def score(self, word:str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        x = np.array(list(sorted(self.word_distribution[word.lower()].copy().values())))\n",
    "        if len(x) > 1:\n",
    "            x = x[x>=np.percentile(x, self.trim_percentile)]\n",
    "            return max(x) / sum(x)\n",
    "        return 1\n",
    "\n",
    "    def text_scores(text:list):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        score = {}\n",
    "        for w in text:\n",
    "            score[w] = score(w)\n",
    "        return score\n",
    "\n",
    "def w2v(w):\n",
    "    try:\n",
    "        v = word2vec[w]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68275edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file labeled_350000.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home | The Bridge DONATE Artboard 1 Student Po...</td>\n",
       "      <td>civic &amp; social organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minimax labs | more from less minimax labs Wha...</td>\n",
       "      <td>computer software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driscoll Creative – Wendy Driscoll | Graphic D...</td>\n",
       "      <td>graphic design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Original Smoothie Bombs™ – The Smoothie Bo...</td>\n",
       "      <td>food &amp; beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Becky Beauchine Kulka | Diamonds &amp; Fine Jewelr...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349994</th>\n",
       "      <td>New York Veterinarian Services | Brooklyn Emer...</td>\n",
       "      <td>veterinary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349995</th>\n",
       "      <td>Home › PeoplesSouth Bank Skip Navigation Downl...</td>\n",
       "      <td>banking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349996</th>\n",
       "      <td>Socii Coffee Skip to content Login Cart / $0.0...</td>\n",
       "      <td>food production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349997</th>\n",
       "      <td>Z Best Dining &amp; Entertainment Coupon Books Ord...</td>\n",
       "      <td>fund-raising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349998</th>\n",
       "      <td>Lecture Programs - Diversity - Women - GLBTQ -...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Home | The Bridge DONATE Artboard 1 Student Po...   \n",
       "1       minimax labs | more from less minimax labs Wha...   \n",
       "2       Driscoll Creative – Wendy Driscoll | Graphic D...   \n",
       "3       The Original Smoothie Bombs™ – The Smoothie Bo...   \n",
       "4       Becky Beauchine Kulka | Diamonds & Fine Jewelr...   \n",
       "...                                                   ...   \n",
       "349994  New York Veterinarian Services | Brooklyn Emer...   \n",
       "349995  Home › PeoplesSouth Bank Skip Navigation Downl...   \n",
       "349996  Socii Coffee Skip to content Login Cart / $0.0...   \n",
       "349997  Z Best Dining & Entertainment Coupon Books Ord...   \n",
       "349998  Lecture Programs - Diversity - Women - GLBTQ -...   \n",
       "\n",
       "                           industry  \n",
       "0       civic & social organization  \n",
       "1                 computer software  \n",
       "2                    graphic design  \n",
       "3                  food & beverages  \n",
       "4                            retail  \n",
       "...                             ...  \n",
       "349994                   veterinary  \n",
       "349995                      banking  \n",
       "349996              food production  \n",
       "349997                 fund-raising  \n",
       "349998                entertainment  \n",
       "\n",
       "[349999 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = 'labeled_350000.csv'\n",
    "\n",
    "print(f'Reading CSV file {labeled_data}')\n",
    "raw = pd.read_csv(labeled_data, usecols=['text', 'industry'], dtype={'text': str, 'industry':str})\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ad37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Home  The Bridge DONATE Artboard  Student Port...\n",
       "1         minimax labs  more from less minimax labs What...\n",
       "2         Driscoll Creative – Wendy Driscoll  Graphic De...\n",
       "3         The Original Smoothie Bombs™ – The Smoothie Bo...\n",
       "4         Becky Beauchine Kulka  Diamonds  Fine Jewelry ...\n",
       "                                ...                        \n",
       "349994    New York Veterinarian Services  Brooklyn Emerg...\n",
       "349995    Home › PeoplesSouth Bank Skip Navigation Downl...\n",
       "349996    Socii Coffee Skip to content Login Cart    No ...\n",
       "349997    Z Best Dining  Entertainment Coupon Books Orde...\n",
       "349998    Lecture Programs  Diversity  Women  GLBTQ Scie...\n",
       "Name: text, Length: 349999, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "CHAR_FILTER_TABLE = str.maketrans('', '', string.punctuation + '©™–' + '0123456789')\n",
    "raw.text.str.translate(CHAR_FILTER_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "160c1c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~©™–0123456789'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation + '©™–' + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa2ad9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Home  The Bridge DONATE Artboard  Student Port...\n",
       "1    minimax labs  more from less minimax labs What...\n",
       "2    Driscoll Creative – Wendy Driscoll  Graphic De...\n",
       "3    The Original Smoothie Bombs™ – The Smoothie Bo...\n",
       "4    Becky Beauchine Kulka  Diamonds  Fine Jewelry ...\n",
       "5    dust Home  dust Archive Dust  –  studiodust A ...\n",
       "6    Vancouver Island Naturopathic Clinic  Victoria...\n",
       "7    Masterman Enterprises  Web Development Home Pr...\n",
       "8    Home  Murfitts Industries    Home PROgran PROt...\n",
       "9    American Paper  Plastic JavaScript seems to be...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head(10).text.str.translate(CHAR_FILTER_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7df0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CHAR_FILTER_TABLE = str.maketrans('', '', string.punctuation + '©™–' + '0123456789')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "WORD_PATTERN = re.compile(r'\\w+')\n",
    "def tokenize(text):\n",
    "    tokens = WORD_PATTERN.findall(text)\n",
    "    return [t for t in tokens if (len(t) > 2 or t.lower() not in STOPWORDS)]\n",
    "\n",
    "def normalize_text_series(text:pd.Series):\n",
    "    text = text.str.translate(CHAR_FILTER_TABLE).str.lower()\n",
    "    text = text.apply(tokenize, convert_dtype=False)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc405aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['normalized'] = normalize_text_series(raw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Normalizing text')\n",
    "raw['normalized'] = raw.text.apply(lambda x:normalize_text(str(x)), convert_dtype=False)\n",
    "\n",
    "print('Calculating word distribution over industries')\n",
    "word_industry = raw.explode('normalized')[['normalized', 'industry']]\n",
    "word_industry['lower'] = word_industry['normalized'].apply(lambda x:x.lower())\n",
    "pair_count = word_industry[['industry', 'lower']].value_counts()\n",
    "word_dist = defaultdict(dict)\n",
    "word_count = defaultdict(int)\n",
    "for (ind, w), count in pair_count.iteritems():\n",
    "    word_dist[w][ind] = count\n",
    "    word_count[w] += count\n",
    "\n",
    "word_score = DomainWordScore(word_dist, 90)\n",
    "_data = pd.DataFrame(raw['normalized'].copy().explode('normalized'))\n",
    "_data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "_data['token_lower'] = _data['token'].apply(lambda x:str(x).lower())\n",
    "_data.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "\n",
    "print('Scoring training data words')\n",
    "_data['score'] = _data['token_lower'].apply(word_score.score)\n",
    "\n",
    "print('Embedding word vectors using pre-trained word2vec')\n",
    "_data['word_vec'] = _data['token_lower'].apply(w2v)\n",
    "\n",
    "print('Ignoring OOV words')\n",
    "_data.dropna(subset=['word_vec'], inplace=True)\n",
    "\n",
    "X = np.array(_data.word_vec.tolist())\n",
    "y = _data.score\n",
    "\n",
    "print('X shape = ', X.shape)\n",
    "print('y shape = ', y.shape)\n",
    "\n",
    "regr = MLPRegressor(\n",
    "    verbose=True, \n",
    "    hidden_layer_sizes=92, \n",
    "    max_iter=300, \n",
    "    tol=1e-5, \n",
    "    learning_rate='adaptive'\n",
    ")\n",
    "\n",
    "print('Training...')\n",
    "regr.fit(X,y)\n",
    "\n",
    "\n",
    "print(f'Saving model locally to file: {model_file_name}')\n",
    "timestamp = str(int(datetime.now().timestamp()))\n",
    "with open(f'model_{timestamp}.sklearn', 'wb') as model_file:\n",
    "    pickle.dump(regr, model_file)\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaffe61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "!pip install scikit-learn==1.0.1\n",
    "!pip install gensim==3.8.3\n",
    "!pip install nltk==3.6.5\n",
    "!pip install matplotlib==3.5.0\n",
    "!pip install beautifulsoup4==4.9.3\n",
    "!pip install numpy==1.19.5\n",
    "!pip install pandas==1.3.4\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "labeled_data = 'labeled_350000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96386fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a33bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3f410",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd7510",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = '<h1>Our company is focused on making the world a better place for Dogs</h1>'\n",
    "tokens = word_tokenize(s)\n",
    "tagged = pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    return bsoup(text,'html.parser').get_text()\n",
    "\n",
    "clean_html('<div> hello <a>aasdasd</a> <img src=\"#\"/><h1>TEXT</h1></div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_table = str.maketrans('', '', '0123456789')\n",
    "\n",
    "def remove_numeric(text:str):\n",
    "    return text.translate(numeric_table)\n",
    "    \n",
    "remove_numeric(' asdas 123123 kjkl123jl4k23j!@#!@#!K23j1kl23j1k23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_table = str.maketrans('', '', string.punctuation + '©')\n",
    "\n",
    "def remove_punc(text:str):\n",
    "    return text.translate(punc_table)\n",
    "    \n",
    "remove_punc('kjkl123jl4k23j!@#!@#!K23j1kl23j1k23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7468f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "print('denied -> ', get_lemma('denied'))\n",
    "print('talked -> ', get_lemma('talked'))\n",
    "print('understood -> ', get_lemma('understood'))\n",
    "print('<<k1lj23 -> ', get_lemma('<<k1lj23'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6427d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = clean_html(text)\n",
    "    text = remove_punc(text)\n",
    "    text = remove_numeric(text)\n",
    "    tokens = word_tokenize(text)\n",
    "#    tokens = [get_lemma(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t.lower() not in stopwords]\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9779e",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ec299",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(labeled_data, usecols=['text', 'industry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3142e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = set(raw.industry)\n",
    "len(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c30bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['text_length'] = raw.text.apply(lambda x:len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e864e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.text_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(raw.text_length, bins=1000)\n",
    "plt.xlim(0,2000)\n",
    "plt.title('Phrase Length Distribution (Before normalization)')\n",
    "plt.xlabel('Number of text')\n",
    "plt.ylabel('Phrase length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be83cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw['normalized'] = raw.text.apply(lambda x:normalize_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['normalized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148704f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_industry = raw.explode('normalized')[['normalized', 'industry']]\n",
    "word_industry['lower'] = word_industry['normalized'].apply(lambda x:x.lower())\n",
    "word_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_count = word_industry[['industry', 'lower']].value_counts()\n",
    "pair_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6388e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_dist = defaultdict(dict)\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "for (ind, w), count in pair_count.iteritems():\n",
    "    word_dist[w][ind] = count\n",
    "    word_count[w] += count\n",
    "\n",
    "w = 'football'\n",
    "print(f'The frequency of the word \"{w}\" in each industry')\n",
    "word_dist[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score(word:str):\n",
    "    x = np.array(list(sorted(word_dist[word.lower()].copy().values())))\n",
    "    if len(x) > 1:\n",
    "        percentile = 0.90 * 100\n",
    "        x = x[x>=np.percentile(x, percentile)]\n",
    "        return max(x) / sum(x)\n",
    "    return 1\n",
    "\n",
    "def text_scores(text:list):\n",
    "    \"\"\"\n",
    "    calculated the score for each word in a text\n",
    "    score - the amount of information we retreive from the word about the industry\n",
    "    \n",
    "    trim the lower frequencies and calc max(y) / sum(y)\n",
    "    \"\"\"\n",
    "    score = {}\n",
    "    for w in text:\n",
    "        score[w] = _score(w)\n",
    "    return score\n",
    "\n",
    "text_scores(raw.normalized[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = pd.DataFrame(raw['normalized'].copy().explode('normalized'))\n",
    "_data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "_data['token_lower'] = _data['token'].apply(lambda x:str(x).lower())\n",
    "_data.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "_data['score'] = _data['token_lower'].apply(_score)\n",
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f66c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    try:\n",
    "        v = word2vec[w]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "_data['word_vec'] = _data['token_lower'].apply(w2v)\n",
    "_data.dropna(subset=['word_vec'], inplace=True)\n",
    "_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e41cd",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Regression\n",
    "\n",
    "Using scikit-learn MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e715d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(_data.word_vec.tolist())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = _data.score\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=92, max_iter=300, verbose=True, tol=1e-5, learning_rate='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd238e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71609ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.sklearn', 'wb') as model_file:\n",
    "    pickle.dump(regr, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf14e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y[:5])\n",
    "print(regr.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c43090",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = raw.text[2000]\n",
    "_x = normalize_text(sample_text)\n",
    "print(\"Number of tokens in the text:\" , len(_x))\n",
    "print(\"Industry = \", raw.industry[2000])\n",
    "print(\"Text\")\n",
    "print(raw.text[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = pd.DataFrame()\n",
    "_X['token'] = _x\n",
    "_X['token_lower'] = _X.token.apply(lambda x:x.lower())\n",
    "_X.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "_X['word_vec'] = _X['token_lower'].apply(w2v)\n",
    "_X.dropna(subset=['word_vec'], inplace=True)\n",
    "_X['score'] = _X.word_vec.apply(lambda _v: float(regr.predict(_v.reshape(1,-1))))\n",
    "_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076385f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X[['token', 'score']].sort_values(by=['score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist['starlight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2857182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    try:\n",
    "        v = word2vec[w]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "data = pd.read_csv('unlabeled.csv', usecols=['id', 'text'], nrows=10)\n",
    "data['normalized'] = data.text.apply(lambda x: normalize_text(str(x)))\n",
    "data = data.explode('normalized')\n",
    "data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "data['token_lower'] = data['token'].apply(lambda x:str(x).lower())\n",
    "data.drop_duplicates(subset=['id', 'token_lower'], inplace=True)\n",
    "data['word_vec'] = data['token_lower'].apply(w2v)\n",
    "data.dropna(subset=['word_vec'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with open('model_temp.sklearn', 'rb') as model_file:\n",
    "    regr = pickle.load(model_file)\n",
    "\n",
    "data['score'] = data.word_vec.apply(lambda _v: float(regr.predict(_v.reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data.groupby(['id'])['score'].nlargest(10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[3].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output = defaultdict(list)\n",
    "\n",
    "for (key, score) in results.items():\n",
    "    doc_id = key[0]\n",
    "    row_id = key[1]\n",
    "    output[doc_id].append(data.iloc[row_id].token)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'id':key, 'snippet':tokens} for key, tokens in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bed750",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head unlabeled.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
