{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7aae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/snirlugassy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/snirlugassy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/snirlugassy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import gensim.downloader as api\n",
    "\n",
    "from processing import normalize_text_series\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68275edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file ../labeled_100000.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>locality</th>\n",
       "      <th>founded</th>\n",
       "      <th>industry</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home | The Bridge DONATE Artboard 1 Student Po...</td>\n",
       "      <td>united states</td>\n",
       "      <td>illinois</td>\n",
       "      <td>orland park</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>civic &amp; social organization</td>\n",
       "      <td>11-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minimax labs | more from less minimax labs Wha...</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>greater london</td>\n",
       "      <td>london</td>\n",
       "      <td>NaN</td>\n",
       "      <td>computer software</td>\n",
       "      <td>1-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Driscoll Creative – Wendy Driscoll | Graphic D...</td>\n",
       "      <td>united states</td>\n",
       "      <td>kentucky</td>\n",
       "      <td>prospect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graphic design</td>\n",
       "      <td>1-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Original Smoothie Bombs™ – The Smoothie Bo...</td>\n",
       "      <td>australia</td>\n",
       "      <td>victoria</td>\n",
       "      <td>port melbourne</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>food &amp; beverages</td>\n",
       "      <td>1-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Becky Beauchine Kulka | Diamonds &amp; Fine Jewelr...</td>\n",
       "      <td>united states</td>\n",
       "      <td>michigan</td>\n",
       "      <td>okemos</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>retail</td>\n",
       "      <td>11-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Paralegal Courses Online | Centre for Paralega...</td>\n",
       "      <td>australia</td>\n",
       "      <td>victoria</td>\n",
       "      <td>melbourne</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>legal services</td>\n",
       "      <td>1-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>eSmartKeep – Keep your numbers green About Us ...</td>\n",
       "      <td>united states</td>\n",
       "      <td>florida</td>\n",
       "      <td>miami</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>financial services</td>\n",
       "      <td>11-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Welcome - Home From Home Kent Comfortable Acco...</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>medway</td>\n",
       "      <td>rochester</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>hospitality</td>\n",
       "      <td>1-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Pearl Technology | Home \\r Services\\r Cybersec...</td>\n",
       "      <td>united states</td>\n",
       "      <td>illinois</td>\n",
       "      <td>peoria heights</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>computer &amp; network security</td>\n",
       "      <td>51-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Global Freight Forwarder | United States | SHI...</td>\n",
       "      <td>united states</td>\n",
       "      <td>arizona</td>\n",
       "      <td>mesa</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>logistics and supply chain</td>\n",
       "      <td>11-50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text         country  \\\n",
       "id                                                                         \n",
       "1      Home | The Bridge DONATE Artboard 1 Student Po...   united states   \n",
       "2      minimax labs | more from less minimax labs Wha...  united kingdom   \n",
       "3      Driscoll Creative – Wendy Driscoll | Graphic D...   united states   \n",
       "4      The Original Smoothie Bombs™ – The Smoothie Bo...       australia   \n",
       "5      Becky Beauchine Kulka | Diamonds & Fine Jewelr...   united states   \n",
       "...                                                  ...             ...   \n",
       "99995  Paralegal Courses Online | Centre for Paralega...       australia   \n",
       "99996  eSmartKeep – Keep your numbers green About Us ...   united states   \n",
       "99997  Welcome - Home From Home Kent Comfortable Acco...  united kingdom   \n",
       "99998  Pearl Technology | Home \\r Services\\r Cybersec...   united states   \n",
       "99999  Global Freight Forwarder | United States | SHI...   united states   \n",
       "\n",
       "               region        locality  founded                     industry  \\\n",
       "id                                                                            \n",
       "1            illinois     orland park   2010.0  civic & social organization   \n",
       "2      greater london          london      NaN            computer software   \n",
       "3            kentucky        prospect      NaN               graphic design   \n",
       "4            victoria  port melbourne   2011.0             food & beverages   \n",
       "5            michigan          okemos   1988.0                       retail   \n",
       "...               ...             ...      ...                          ...   \n",
       "99995        victoria       melbourne   1996.0               legal services   \n",
       "99996         florida           miami   2018.0           financial services   \n",
       "99997          medway       rochester   2018.0                  hospitality   \n",
       "99998        illinois  peoria heights   1998.0  computer & network security   \n",
       "99999         arizona            mesa   2011.0   logistics and supply chain   \n",
       "\n",
       "         size  \n",
       "id             \n",
       "1       11-50  \n",
       "2        1-10  \n",
       "3        1-10  \n",
       "4        1-10  \n",
       "5       11-50  \n",
       "...       ...  \n",
       "99995    1-10  \n",
       "99996   11-50  \n",
       "99997    1-10  \n",
       "99998  51-200  \n",
       "99999   11-50  \n",
       "\n",
       "[99999 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = '../labeled_100000.csv'\n",
    "\n",
    "data_types = {\n",
    "    'id': np.int64,\n",
    "    'text': str,\n",
    "    'country': str,\n",
    "    'region': str,\n",
    "    'locality': str,\n",
    "    'founded': np.float,\n",
    "    'industry': str,\n",
    "    'size': str\n",
    "}\n",
    "\n",
    "print(f'Reading CSV file {labeled_data}')\n",
    "data = pd.read_csv(labeled_data, dtype=data_types, index_col='id')\n",
    "data.text.replace(np.nan, \"\", inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb28a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = data.industry.unique()\n",
    "num_of_industries = len(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09df1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = data.text\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "228cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, Birch, SpectralClustering\n",
    "\n",
    "clustering = Birch(n_clusters=20)\n",
    "X_cluster_dist = clustering.fit_transform(X)\n",
    "\n",
    "with open(\"clustering.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clustering, f)\n",
    "\n",
    "pred = clustering.predict(X)\n",
    "data['cluster'] = pred\n",
    "\n",
    "cluster_ind = data[['cluster', 'industry']].groupby(['industry']).agg(lambda x:x.value_counts().index[0])\n",
    "cluster_ind = cluster_ind.cluster.to_dict()\n",
    "\n",
    "_cluster_dist = np.zeros(20)\n",
    "for i,c in cluster_ind.items():\n",
    "    _cluster_dist[c] += 1\n",
    "_cluster_dist /= sum(_cluster_dist)\n",
    "print('cluster size in %: \\n' , _cluster_dist*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e4ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (/home/snirlugassy/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:301742)",
      "at S.execute (/home/snirlugassy/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:300732)",
      "at S.start (/home/snirlugassy/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/snirlugassy/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (/home/snirlugassy/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "_cluster_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = normalize_text_series(data.text)\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [t for t in tokens if t.lower() not in STOPWORDS])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.industry.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ad9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_industry = data.explode('tokens')[['tokens', 'industry']]\n",
    "word_industry.rename(columns={'tokens':'token'}, inplace=True)\n",
    "word_industry.token = word_industry.token.str.lower()\n",
    "word_industry.reset_index(inplace=True, drop=True)\n",
    "word_industry\n",
    "# word_industry['lower'] = word_industry['text'].apply(lambda x: str(x).lower())\n",
    "# pair_count = word_industry[['industry', 'lower']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c56ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist = defaultdict(dict)\n",
    "word_count = defaultdict(int)\n",
    "for (ind, w), count in word_industry.value_counts().iteritems():\n",
    "    word_dist[w][ind] = count\n",
    "    word_count[w] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document???.',\n",
    "    'This document &is the !!!second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words=STOPWORDS)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0ba82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30d659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf03c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13856cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CHAR_FILTER_TABLE = str.maketrans('', '', string.punctuation + '©™–' + '0123456789')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "WORD_PATTERN = re.compile(r'\\w+')\n",
    "def tokenize(text):\n",
    "    tokens = WORD_PATTERN.findall(text)\n",
    "    return [t for t in tokens if (len(t) > 2 or t.lower() not in STOPWORDS)]\n",
    "\n",
    "def normalize_text_series(text:pd.Series):\n",
    "    text = text.str.translate(CHAR_FILTER_TABLE).str.lower()\n",
    "    text = text.apply(tokenize, convert_dtype=False)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc405aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['normalized'] = normalize_text_series(raw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Normalizing text')\n",
    "raw['normalized'] = raw.text.apply(lambda x:normalize_text(str(x)), convert_dtype=False)\n",
    "\n",
    "print('Calculating word distribution over industries')\n",
    "word_industry = raw.explode('normalized')[['normalized', 'industry']]\n",
    "word_industry['lower'] = word_industry['normalized'].apply(lambda x:x.lower())\n",
    "pair_count = word_industry[['industry', 'lower']].value_counts()\n",
    "word_dist = defaultdict(dict)\n",
    "word_count = defaultdict(int)\n",
    "for (ind, w), count in pair_count.iteritems():\n",
    "    word_dist[w][ind] = count\n",
    "    word_count[w] += count\n",
    "\n",
    "word_score = DomainWordScore(word_dist, 90)\n",
    "_data = pd.DataFrame(raw['normalized'].copy().explode('normalized'))\n",
    "_data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "_data['token_lower'] = _data['token'].apply(lambda x:str(x).lower())\n",
    "_data.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "\n",
    "print('Scoring training data words')\n",
    "_data['score'] = _data['token_lower'].apply(word_score.score)\n",
    "\n",
    "print('Embedding word vectors using pre-trained word2vec')\n",
    "_data['word_vec'] = _data['token_lower'].apply(w2v)\n",
    "\n",
    "print('Ignoring OOV words')\n",
    "_data.dropna(subset=['word_vec'], inplace=True)\n",
    "\n",
    "X = np.array(_data.word_vec.tolist())\n",
    "y = _data.score\n",
    "\n",
    "print('X shape = ', X.shape)\n",
    "print('y shape = ', y.shape)\n",
    "\n",
    "regr = MLPRegressor(\n",
    "    verbose=True, \n",
    "    hidden_layer_sizes=92, \n",
    "    max_iter=300, \n",
    "    tol=1e-5, \n",
    "    learning_rate='adaptive'\n",
    ")\n",
    "\n",
    "print('Training...')\n",
    "regr.fit(X,y)\n",
    "\n",
    "\n",
    "print(f'Saving model locally to file: {model_file_name}')\n",
    "timestamp = str(int(datetime.now().timestamp()))\n",
    "with open(f'model_{timestamp}.sklearn', 'wb') as model_file:\n",
    "    pickle.dump(regr, model_file)\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaffe61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "!pip install scikit-learn==1.0.1\n",
    "!pip install gensim==3.8.3\n",
    "!pip install nltk==3.6.5\n",
    "!pip install matplotlib==3.5.0\n",
    "!pip install beautifulsoup4==4.9.3\n",
    "!pip install numpy==1.19.5\n",
    "!pip install pandas==1.3.4\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "labeled_data = 'labeled_350000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96386fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a33bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3f410",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd7510",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = '<h1>Our company is focused on making the world a better place for Dogs</h1>'\n",
    "tokens = word_tokenize(s)\n",
    "tagged = pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    return bsoup(text,'html.parser').get_text()\n",
    "\n",
    "clean_html('<div> hello <a>aasdasd</a> <img src=\"#\"/><h1>TEXT</h1></div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_table = str.maketrans('', '', '0123456789')\n",
    "\n",
    "def remove_numeric(text:str):\n",
    "    return text.translate(numeric_table)\n",
    "    \n",
    "remove_numeric(' asdas 123123 kjkl123jl4k23j!@#!@#!K23j1kl23j1k23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_table = str.maketrans('', '', string.punctuation + '©')\n",
    "\n",
    "def remove_punc(text:str):\n",
    "    return text.translate(punc_table)\n",
    "    \n",
    "remove_punc('kjkl123jl4k23j!@#!@#!K23j1kl23j1k23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7468f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "print('denied -> ', get_lemma('denied'))\n",
    "print('talked -> ', get_lemma('talked'))\n",
    "print('understood -> ', get_lemma('understood'))\n",
    "print('<<k1lj23 -> ', get_lemma('<<k1lj23'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6427d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = clean_html(text)\n",
    "    text = remove_punc(text)\n",
    "    text = remove_numeric(text)\n",
    "    tokens = word_tokenize(text)\n",
    "#    tokens = [get_lemma(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t.lower() not in stopwords]\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9779e",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ec299",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(labeled_data, usecols=['text', 'industry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3142e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = set(raw.industry)\n",
    "len(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c30bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['text_length'] = raw.text.apply(lambda x:len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e864e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.text_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(raw.text_length, bins=1000)\n",
    "plt.xlim(0,2000)\n",
    "plt.title('Phrase Length Distribution (Before normalization)')\n",
    "plt.xlabel('Number of text')\n",
    "plt.ylabel('Phrase length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be83cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw['normalized'] = raw.text.apply(lambda x:normalize_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['normalized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148704f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_industry = raw.explode('normalized')[['normalized', 'industry']]\n",
    "word_industry['lower'] = word_industry['normalized'].apply(lambda x:x.lower())\n",
    "word_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_count = word_industry[['industry', 'lower']].value_counts()\n",
    "pair_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6388e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_dist = defaultdict(dict)\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "for (ind, w), count in pair_count.iteritems():\n",
    "    word_dist[w][ind] = count\n",
    "    word_count[w] += count\n",
    "\n",
    "w = 'football'\n",
    "print(f'The frequency of the word \"{w}\" in each industry')\n",
    "word_dist[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score(word:str):\n",
    "    x = np.array(list(sorted(word_dist[word.lower()].copy().values())))\n",
    "    if len(x) > 1:\n",
    "        percentile = 0.90 * 100\n",
    "        x = x[x>=np.percentile(x, percentile)]\n",
    "        return max(x) / sum(x)\n",
    "    return 1\n",
    "\n",
    "def text_scores(text:list):\n",
    "    \"\"\"\n",
    "    calculated the score for each word in a text\n",
    "    score - the amount of information we retreive from the word about the industry\n",
    "    \n",
    "    trim the lower frequencies and calc max(y) / sum(y)\n",
    "    \"\"\"\n",
    "    score = {}\n",
    "    for w in text:\n",
    "        score[w] = _score(w)\n",
    "    return score\n",
    "\n",
    "text_scores(raw.normalized[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = pd.DataFrame(raw['normalized'].copy().explode('normalized'))\n",
    "_data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "_data['token_lower'] = _data['token'].apply(lambda x:str(x).lower())\n",
    "_data.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "_data['score'] = _data['token_lower'].apply(_score)\n",
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f66c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    try:\n",
    "        v = word2vec[w]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "_data['word_vec'] = _data['token_lower'].apply(w2v)\n",
    "_data.dropna(subset=['word_vec'], inplace=True)\n",
    "_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e41cd",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Regression\n",
    "\n",
    "Using scikit-learn MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e715d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(_data.word_vec.tolist())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = _data.score\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=92, max_iter=300, verbose=True, tol=1e-5, learning_rate='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd238e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71609ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.sklearn', 'wb') as model_file:\n",
    "    pickle.dump(regr, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf14e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y[:5])\n",
    "print(regr.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c43090",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = raw.text[2000]\n",
    "_x = normalize_text(sample_text)\n",
    "print(\"Number of tokens in the text:\" , len(_x))\n",
    "print(\"Industry = \", raw.industry[2000])\n",
    "print(\"Text\")\n",
    "print(raw.text[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = pd.DataFrame()\n",
    "_X['token'] = _x\n",
    "_X['token_lower'] = _X.token.apply(lambda x:x.lower())\n",
    "_X.drop_duplicates(subset=['token_lower'], inplace=True)\n",
    "_X['word_vec'] = _X['token_lower'].apply(w2v)\n",
    "_X.dropna(subset=['word_vec'], inplace=True)\n",
    "_X['score'] = _X.word_vec.apply(lambda _v: float(regr.predict(_v.reshape(1,-1))))\n",
    "_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076385f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X[['token', 'score']].sort_values(by=['score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist['starlight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2857182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    try:\n",
    "        v = word2vec[w]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "data = pd.read_csv('unlabeled.csv', usecols=['id', 'text'], nrows=10)\n",
    "data['normalized'] = data.text.apply(lambda x: normalize_text(str(x)))\n",
    "data = data.explode('normalized')\n",
    "data.rename(columns={'normalized': 'token'}, inplace=True)\n",
    "data['token_lower'] = data['token'].apply(lambda x:str(x).lower())\n",
    "data.drop_duplicates(subset=['id', 'token_lower'], inplace=True)\n",
    "data['word_vec'] = data['token_lower'].apply(w2v)\n",
    "data.dropna(subset=['word_vec'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with open('model_temp.sklearn', 'rb') as model_file:\n",
    "    regr = pickle.load(model_file)\n",
    "\n",
    "data['score'] = data.word_vec.apply(lambda _v: float(regr.predict(_v.reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data.groupby(['id'])['score'].nlargest(10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[3].token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output = defaultdict(list)\n",
    "\n",
    "for (key, score) in results.items():\n",
    "    doc_id = key[0]\n",
    "    row_id = key[1]\n",
    "    output[doc_id].append(data.iloc[row_id].token)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'id':key, 'snippet':tokens} for key, tokens in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bed750",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head unlabeled.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
